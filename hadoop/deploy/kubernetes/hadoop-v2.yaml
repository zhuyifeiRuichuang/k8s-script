apiVersion: v1
kind: Namespace
metadata:
  name: bigdata2
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: hadoop-config
  namespace: bigdata2
data:
  core-site.xml: |
    <?xml version="1.0" encoding="UTF-8"?>
    <configuration>
      <property>
        <name>fs.default.name</name>
        <value>hdfs://namenode:8020</value>
      </property>
      <property>
        <name>fs.defaultFS</name>
        <value>hdfs://namenode:8020</value>
      </property>
    </configuration>
  hdfs-site.xml: |
    <?xml version="1.0" encoding="UTF-8"?>
    <configuration>
      <property>
        <name>dfs.namenode.rpc-address</name>
        <value>namenode:8020</value>
      </property>
      <property>
        <name>dfs.replication</name>
        <value>3</value>
      </property>
      <property>
        <name>dfs.namenode.name.dir</name>
        <value>/tmp/hadoop-root/dfs/name</value>
      </property>
      <property>
        <name>dfs.datanode.data.dir</name>
        <value>/tmp/hadoop-root/dfs/data</value>
      </property>
      <property>
        <name>dfs.permissions.enabled</name>
        <value>false</value>
      </property>
      <property>
        <name>dfs.client.use.datanode.hostname</name>
        <value>true</value>
      </property>
      <property>
        <name>dfs.datanode.use.datanode.hostname</name>
        <value>true</value>
      </property>
    </configuration>
  mapred-site.xml: |
    <?xml version="1.0" encoding="UTF-8"?>
    <configuration>
      <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
      </property>
      <property>
        <name>yarn.app.mapreduce.am.env</name>
        <value>HADOOP_MAPRED_HOME=/opt/hadoop</value>
      </property>
      <property>
        <name>mapreduce.map.env</name>
        <value>HADOOP_MAPRED_HOME=/opt/hadoop</value>
      </property>
      <property>
        <name>mapreduce.reduce.env</name>
        <value>HADOOP_MAPRED_HOME=/opt/hadoop</value>
      </property>
    </configuration>
  yarn-site.xml: |
    <?xml version="1.0" encoding="UTF-8"?>
    <configuration>
      <property>
        <name>yarn.resourcemanager.hostname</name>
        <value>resourcemanager</value>
      </property>
      <property>
        <name>yarn.nodemanager.pmem-check-enabled</name>
        <value>false</value>
      </property>
      <property>
        <name>yarn.nodemanager.delete.debug-delay-sec</name>
        <value>600</value>
      </property>
      <property>
        <name>yarn.nodemanager.vmem-check-enabled</name>
        <value>false</value>
      </property>
      <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
      </property>
      <property>
        <name>yarn.resourcemanager.address</name>
        <value>resourcemanager:8032</value>
      </property>
    </configuration>
  capacity-scheduler.xml: |
    <?xml version="1.0" encoding="UTF-8"?>
    <configuration>
      <property>
        <name>yarn.scheduler.capacity.maximum-applications</name>
        <value>10000</value>
      </property>
      <property>
        <name>yarn.scheduler.capacity.maximum-am-resource-percent</name>
        <value>0.1</value>
      </property>
      <property>
        <name>yarn.scheduler.capacity.resource-calculator</name>
        <value>org.apache.hadoop.yarn.util.resource.DefaultResourceCalculator</value>
      </property>
      <property>
        <name>yarn.scheduler.capacity.root.queues</name>
        <value>default</value>
      </property>
      <property>
        <name>yarn.scheduler.capacity.root.default.capacity</name>
        <value>100</value>
      </property>
      <property>
        <name>yarn.scheduler.capacity.root.default.user-limit-factor</name>
        <value>1</value>
      </property>
      <property>
        <name>yarn.scheduler.capacity.root.default.maximum-capacity</name>
        <value>100</value>
      </property>
      <property>
        <name>yarn.scheduler.capacity.root.default.state</name>
        <value>RUNNING</value>
      </property>
      <property>
        <name>yarn.scheduler.capacity.root.default.acl_submit_applications</name>
        <value>*</value>
      </property>
      <property>
        <name>yarn.scheduler.capacity.root.default.acl_administer_queue</name>
        <value>*</value>
      </property>
      <property>
        <name>yarn.scheduler.capacity.node-locality-delay</name>
        <value>40</value>
      </property>
      <property>
        <name>yarn.scheduler.capacity.queue-mappings</name>
        <value></value>
      </property>
      <property>
        <name>yarn.scheduler.capacity.queue-mappings-override.enable</name>
        <value>false</value>
      </property>
    </configuration>
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: hadoop-test-script
  namespace: bigdata2
data:
  test.sh: |
    #!/bin/bash
    echo "Hadoop ResourceManager Test Script"
    echo "Current Hadoop Version: 3.1.1"
    echo "HDFS Status: $(hdfs dfsadmin -report | grep 'DFS Used%')"
    echo "YARN Status: $(yarn node -list)"
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: namenode-pvc
  namespace: bigdata2
spec:
  accessModes: [ "ReadWriteOnce" ]
  storageClassName: "local"
  resources:
    requests:
      storage: 10Gi
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: resourcemanager-pvc
  namespace: bigdata2
spec:
  accessModes: [ "ReadWriteOnce" ]
  storageClassName: "local"
  resources:
    requests:
      storage: 10Gi
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: nodemanager-pvc
  namespace: bigdata2
spec:
  accessModes: [ "ReadWriteOnce" ]
  storageClassName: "local"
  resources:
    requests:
      storage: 10Gi
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: namenode
  namespace: bigdata2
  labels:
    app: hadoop
    component: namenode
spec:
  replicas: 1
  selector:
    matchLabels:
      app: hadoop
      component: namenode
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app: hadoop
        component: namenode
    spec:
      hostname: namenode
      subdomain: hadoop
      containers:
      - name: namenode
        image: ccr.ccs.tencentyun.com/hadoop-dev/hadoop:3.1.1
        securityContext:
          runAsUser: 0
          runAsGroup: 0
        command: ["/bin/sh", "-c"]
        args:
          - |
            set -e
            mkdir -p /tmp/hadoop-root/dfs/name
            chmod -R 777 /tmp/hadoop-root/
            if [ ! -d "/tmp/hadoop-root/dfs/name/current" ]; then
              echo "=== 首次启动，格式化Namenode ==="
              hdfs namenode -format -force -nonInteractive || { echo "格式化失败"; exit 1; }
            fi
            echo "=== 启动Namenode ==="
            hdfs namenode
        ports:
        - containerPort: 9870
        - containerPort: 8020
        - containerPort: 9868
        env:
          - name: HADOOP_HOME
            value: /opt/hadoop
          - name: PATH
            value: "$PATH:/opt/hadoop/bin:/opt/hadoop/sbin:/usr/bin"
          - name: HADOOP_LOG_DIR
            value: /tmp/hadoop-logs
          - name: HADOOP_OPTS
            value: "-Djava.net.preferIPv4Stack=true"
        volumeMounts:
        - name: hadoop-config
          mountPath: /opt/hadoop/etc/hadoop/core-site.xml
          subPath: core-site.xml
          readOnly: true
        - name: hadoop-config
          mountPath: /opt/hadoop/etc/hadoop/hdfs-site.xml
          subPath: hdfs-site.xml
          readOnly: true
        - name: hadoop-config
          mountPath: /opt/hadoop/etc/hadoop/mapred-site.xml
          subPath: mapred-site.xml
          readOnly: true
        - name: hadoop-config
          mountPath: /opt/hadoop/etc/hadoop/yarn-site.xml
          subPath: yarn-site.xml
          readOnly: true
        - name: hadoop-config
          mountPath: /opt/hadoop/etc/hadoop/capacity-scheduler.xml
          subPath: capacity-scheduler.xml
          readOnly: true
        - name: namenode-data
          mountPath: /tmp/hadoop-root
        - name: namenode-logs
          mountPath: /tmp/hadoop-logs
        livenessProbe:
          exec:
            command: ["hdfs", "dfsadmin", "-report"]
          initialDelaySeconds: 180
          periodSeconds: 15
          timeoutSeconds: 5
          failureThreshold: 3
        readinessProbe:
          exec:
            command: ["hdfs", "dfsadmin", "-safemode", "get"]
          initialDelaySeconds: 120
          periodSeconds: 10
          timeoutSeconds: 5
          successThreshold: 1
      volumes:
      - name: hadoop-config
        configMap:
          name: hadoop-config
          defaultMode: 0755
      - name: namenode-data
        persistentVolumeClaim:
          claimName: namenode-pvc
      - name: namenode-logs
        emptyDir: {}
---
# Namenode Service（移除所有nodePort）
apiVersion: v1
kind: Service
metadata:
  name: namenode
  namespace: bigdata2
spec:
  type: NodePort
  selector:
    app: hadoop
    component: namenode
  ports:
  - name: webui
    port: 9870
    targetPort: 9870
  - name: rpc
    port: 8020
    targetPort: 8020
  - name: datanode-http
    port: 9868
    targetPort: 9868
---
# DataNode Headless Service（不变，无NodePort）
apiVersion: v1
kind: Service
metadata:
  name: datanode-headless
  namespace: bigdata2
spec:
  clusterIP: None
  selector:
    app: hadoop
    component: datanode
  ports:
  - name: webui
    port: 9864
    targetPort: 9864
  - name: data-transfer
    port: 9866
    targetPort: 9866
  - name: ipc
    port: 9867
    targetPort: 9867
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: datanode
  namespace: bigdata2
  labels:
    app: hadoop
    component: datanode
spec:
  serviceName: datanode-headless
  replicas: 3
  selector:
    matchLabels:
      app: hadoop
      component: datanode
  template:
    metadata:
      labels:
        app: hadoop
        component: datanode
    spec:
      containers:
      - name: datanode
        image: ccr.ccs.tencentyun.com/hadoop-dev/hadoop:3.1.1
        securityContext:
          runAsUser: 0
          runAsGroup: 0
        command: ["/bin/sh", "-c"]
        args:
          - |
            set -e
            for i in {1..60}; do
              if nc -z namenode 8020; then
                echo "Namenode已就绪！"
                break
              fi
              if [ $i -eq 60 ]; then
                echo "Namenode超时未就绪，退出！"
                exit 1
              fi
              echo "Namenode未就绪，等待10秒..."
              sleep 10
            done
            mkdir -p /tmp/hadoop-root/dfs/data
            chmod -R 777 /tmp/hadoop-root/
            echo "=== 启动DataNode（Pod名称：$(hostname)） ==="
            hdfs datanode
        ports:
        - containerPort: 9864
        - containerPort: 9866
        - containerPort: 9867
        env:
          - name: HADOOP_HOME
            value: /opt/hadoop
          - name: PATH
            value: "$PATH:/opt/hadoop/bin:/opt/hadoop/sbin:/usr/bin"
          - name: HADOOP_LOG_DIR
            value: /tmp/hadoop-logs
          - name: HADOOP_OPTS
            value: "-Djava.net.preferIPv4Stack=true -Ddfs.datanode.hostname=$(hostname)"
        volumeMounts:
        - name: hadoop-config
          mountPath: /opt/hadoop/etc/hadoop/core-site.xml
          subPath: core-site.xml
          readOnly: true
        - name: hadoop-config
          mountPath: /opt/hadoop/etc/hadoop/hdfs-site.xml
          subPath: hdfs-site.xml
          readOnly: true
        - name: hadoop-config
          mountPath: /opt/hadoop/etc/hadoop/mapred-site.xml
          subPath: mapred-site.xml
          readOnly: true
        - name: hadoop-config
          mountPath: /opt/hadoop/etc/hadoop/yarn-site.xml
          subPath: yarn-site.xml
          readOnly: true
        - name: hadoop-config
          mountPath: /opt/hadoop/etc/hadoop/capacity-scheduler.xml
          subPath: capacity-scheduler.xml
          readOnly: true
        - name: datanode-data
          mountPath: /tmp/hadoop-root
        - name: datanode-logs
          mountPath: /tmp/hadoop-logs
        livenessProbe:
          exec:
            command: ["hdfs", "dfsadmin", "-report"]
          initialDelaySeconds: 180
          periodSeconds: 15
          timeoutSeconds: 5
        readinessProbe:
          exec:
            command: ["sh", "-c", "hdfs dfsadmin -report | grep -q $(hostname)"]
          initialDelaySeconds: 120
          periodSeconds: 10
          timeoutSeconds: 5
  volumeClaimTemplates:
  - metadata:
      name: datanode-data
    spec:
      accessModes: [ "ReadWriteOnce" ]
      storageClassName: "local"
      resources:
        requests:
          storage: 10Gi
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: resourcemanager
  namespace: bigdata2
  labels:
    app: hadoop
    component: resourcemanager
spec:
  replicas: 1
  selector:
    matchLabels:
      app: hadoop
      component: resourcemanager
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app: hadoop
        component: resourcemanager
    spec:
      hostname: resourcemanager
      subdomain: hadoop
      containers:
      - name: resourcemanager
        image: ccr.ccs.tencentyun.com/hadoop-dev/hadoop:3.1.1
        securityContext:
          runAsUser: 0
          runAsGroup: 0
        command: ["/bin/sh", "-c"]
        args:
          - |
            set -e
            for i in {1..60}; do
              if nc -z namenode 8020; then
                echo "Namenode已就绪！"
                break
              fi
              if [ $i -eq 60 ]; then
                echo "Namenode超时未就绪，退出！"
                exit 1
              fi
              echo "Namenode未就绪，等待10秒..."
              sleep 10
            done
            mkdir -p /tmp/hadoop-yarn
            chmod -R 777 /tmp/hadoop-yarn
            echo "=== 启动ResourceManager ==="
            yarn resourcemanager
        ports:
        - containerPort: 8088
        - containerPort: 8030
        - containerPort: 8031
        - containerPort: 8032
        env:
          - name: HADOOP_HOME
            value: /opt/hadoop
          - name: PATH
            value: "$PATH:/opt/hadoop/bin:/opt/hadoop/sbin:/usr/bin"
          - name: HADOOP_LOG_DIR
            value: /tmp/hadoop-logs
          - name: HADOOP_OPTS
            value: "-Djava.net.preferIPv4Stack=true"
        volumeMounts:
        - name: hadoop-config
          mountPath: /opt/hadoop/etc/hadoop/core-site.xml
          subPath: core-site.xml
          readOnly: true
        - name: hadoop-config
          mountPath: /opt/hadoop/etc/hadoop/hdfs-site.xml
          subPath: hdfs-site.xml
          readOnly: true
        - name: hadoop-config
          mountPath: /opt/hadoop/etc/hadoop/mapred-site.xml
          subPath: mapred-site.xml
          readOnly: true
        - name: hadoop-config
          mountPath: /opt/hadoop/etc/hadoop/yarn-site.xml
          subPath: yarn-site.xml
          readOnly: true
        - name: hadoop-config
          mountPath: /opt/hadoop/etc/hadoop/capacity-scheduler.xml
          subPath: capacity-scheduler.xml
          readOnly: true
        - name: resourcemanager-data
          mountPath: /tmp/hadoop-yarn
        - name: test-script
          mountPath: /opt/test.sh
          subPath: test.sh
          readOnly: false
        - name: resourcemanager-logs
          mountPath: /tmp/hadoop-logs
        livenessProbe:
          exec:
            command: ["yarn", "node", "-list"]
          initialDelaySeconds: 180
          periodSeconds: 15
          timeoutSeconds: 5
        readinessProbe:
          exec:
            command: ["yarn", "node", "-list"]
          initialDelaySeconds: 120
          periodSeconds: 10
          timeoutSeconds: 5
      volumes:
      - name: hadoop-config
        configMap:
          name: hadoop-config
          defaultMode: 0755
      - name: resourcemanager-data
        persistentVolumeClaim:
          claimName: resourcemanager-pvc
      - name: test-script
        configMap:
          name: hadoop-test-script
          defaultMode: 0755
      - name: resourcemanager-logs
        emptyDir: {}
---
# ResourceManager Service（移除所有nodePort）
apiVersion: v1
kind: Service
metadata:
  name: resourcemanager
  namespace: bigdata2
spec:
  type: NodePort
  selector:
    app: hadoop
    component: resourcemanager
  ports:
  - name: webui
    port: 8088
    targetPort: 8088
  - name: rpc
    port: 8030
    targetPort: 8030
  - name: scheduler
    port: 8031
    targetPort: 8031
  - name: yarn-rpc
    port: 8032
    targetPort: 8032
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nodemanager
  namespace: bigdata2
  labels:
    app: hadoop
    component: nodemanager
spec:
  replicas: 1
  selector:
    matchLabels:
      app: hadoop
      component: nodemanager
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  template:
    metadata:
      labels:
        app: hadoop
        component: nodemanager
    spec:
      containers:
      - name: nodemanager
        image: ccr.ccs.tencentyun.com/hadoop-dev/hadoop:3.1.1
        securityContext:
          runAsUser: 0
          runAsGroup: 0
        command: ["/bin/sh", "-c"]
        args:
          - |
            set -e
            for i in {1..60}; do
              if nc -z resourcemanager 8032; then
                echo "ResourceManager已就绪！"
                break
              fi
              if [ $i -eq 60 ]; then
                echo "ResourceManager超时未就绪，退出！"
                exit 1
              fi
              echo "ResourceManager未就绪，等待10秒..."
              sleep 10
            done
            mkdir -p /tmp/hadoop-yarn-nm
            chmod -R 777 /tmp/hadoop-yarn-nm
            echo "=== 启动NodeManager ==="
            yarn nodemanager
        ports:
        - containerPort: 8042
        - containerPort: 8040
        env:
          - name: HADOOP_HOME
            value: /opt/hadoop
          - name: PATH
            value: "$PATH:/opt/hadoop/bin:/opt/hadoop/sbin:/usr/bin"
          - name: HADOOP_LOG_DIR
            value: /tmp/hadoop-logs
          - name: HADOOP_OPTS
            value: "-Djava.net.preferIPv4Stack=true"
        volumeMounts:
        - name: hadoop-config
          mountPath: /opt/hadoop/etc/hadoop/core-site.xml
          subPath: core-site.xml
          readOnly: true
        - name: hadoop-config
          mountPath: /opt/hadoop/etc/hadoop/hdfs-site.xml
          subPath: hdfs-site.xml
          readOnly: true
        - name: hadoop-config
          mountPath: /opt/hadoop/etc/hadoop/mapred-site.xml
          subPath: mapred-site.xml
          readOnly: true
        - name: hadoop-config
          mountPath: /opt/hadoop/etc/hadoop/yarn-site.xml
          subPath: yarn-site.xml
          readOnly: true
        - name: hadoop-config
          mountPath: /opt/hadoop/etc/hadoop/capacity-scheduler.xml
          subPath: capacity-scheduler.xml
          readOnly: true
        - name: nodemanager-data
          mountPath: /tmp/hadoop-yarn-nm
        - name: nodemanager-logs
          mountPath: /tmp/hadoop-logs
        livenessProbe:
          tcpSocket:
            port: 8040
          initialDelaySeconds: 240
          periodSeconds: 20
        readinessProbe:
          tcpSocket:
            port: 8042
          initialDelaySeconds: 30
          periodSeconds: 5
          timeoutSeconds: 3
          successThreshold: 1
      volumes:
      - name: hadoop-config
        configMap:
          name: hadoop-config
          defaultMode: 0755
      - name: nodemanager-data
        persistentVolumeClaim:
          claimName: nodemanager-pvc
      - name: nodemanager-logs
        emptyDir: {}
---
# NodeManager Service（移除所有nodePort）
apiVersion: v1
kind: Service
metadata:
  name: nodemanager
  namespace: bigdata2
spec:
  type: NodePort
  selector:
    app: hadoop
    component: nodemanager
  ports:
  - name: webui
    port: 8042
    targetPort: 8042
  - name: localizer
    port: 8040
    targetPort: 8040
